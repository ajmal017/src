{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference between supervised and unsupervised learning?\n",
    "\n",
    "- Supervised learning - train labelled dataset with inputs and known outcomes\n",
    "- Unsupervised learning - input data doesn't have labelled outputs\n",
    "\n",
    "### What is the bias-variance trade off?\n",
    "\n",
    "Start by defining:\n",
    "- **Bias**: this is error introduced to the model due to the\n",
    "**oversimplification** of the ML algorithm. It can lead to underfitting, training the model at the time leads to simplified assumptions to make the target function easier to understand.\n",
    "- **Variance**: error introduced in the model due to a **complex** ML algo, the model ends up learning noise from the dataset, performing badly on test data. This can lead to high *sensitivity (true positive rate/probability of detection)* and overfitting.\n",
    "\n",
    "As you increase the complexity of the model, you see a reduction in error due to low bias until a particular point. As the model gets more and more complex, you end up overfitting and therefore getting high variance.\n",
    "\n",
    "The goal of (supervised) ML is low bias and low variance to enable accurate prediction performance.\n",
    "- **kNN** has high variance and low bias. By increasing *k* you increase the number of neighbours contributing to the prediction, therefore increasing model bias\n",
    "- **SVM** has low bias and high variance. As you increase the C parameter, it increases the number of violations of margin allowed, therefore increasing bias and decreasing variance (fewer points' input go into the model).\n",
    "\n",
    "Therefore:\n",
    "> **Increasing BIAS reduces VARIANCE and vice versa**\n",
    "\n",
    "- Low bias ML algo: Decision Trees, kNN, SVM\n",
    "- High bias ML algo: linear regression, logistic regression\n",
    "\n",
    "## What is a confusion matrix?\n",
    "\n",
    "![confusionMatrix.jpg](attachment:confusionMatrix.jpg)\n",
    "\n",
    "Therefore:\n",
    "- **sensitivity** is *true positive rate*\n",
    "- **precision** is the *positive predicted values*\n",
    "- **specificity** is the *true negative rate*\n",
    "\n",
    "### What is an ROC curve?\n",
    "\n",
    "This is a graphical representation of the contrast between true positive and false positive rates at various thresholds.\n",
    "It's often used as a proxy for sensitivity (true positive rate) and false positive rate.\n",
    "\n",
    "### What is SVM?\n",
    "\n",
    "**Support vector machines** is an ML algo used for both *regression* and *classification*.\n",
    "If you have *n* features in a training set, SVM plots these in n-dimensional space with the value of the feature being the particular coordinate.\n",
    "SVM uses hyperplanes to separate the different classes based on the provided kernel function.\n",
    "\n",
    "The **support vectors** are the lines marking the distance from the classifier (separating hyperplane) to the closest datapoints.\n",
    "\n",
    "![svm.png](attachment:svm.png)\n",
    "\n",
    "### Explain the Decision Tree algorithm\n",
    "\n",
    "This is a supervised learning algortihm used for **regression** and **classification**.\n",
    "It is the process of breaking down the datasets into smaller and smaller subsets whilst developing a decision tree incrementally.\n",
    "The final result is a tree with decision and leaf nodes. Both numerical and categorical data can be handled.\n",
    "\n",
    "Removing sub-nodes of a decision tree is called **pruning**.\n",
    "\n",
    "### Explain the Random Forest algorithm\n",
    "\n",
    "This is an ML algo for **regression** and **classification**. \n",
    "\n",
    "It is used for dimensionality reduction for missing values treating outliers. It is an ensemble learning method - weak methods combine to form a powerful model.\n",
    "\n",
    "In a random forest, grow multiple trees as opposed to a single tree. To classify a new object based on attributes, each tree gives a classification. \n",
    "\n",
    "The forest then chooses the classification with the most votes (compared to all trees in the forest), and for regression it takes the average of the outputs by the different trees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "\n",
    "### Explain what a normal distribution is\n",
    "\n",
    "Data distributed by a continuous probability distribution (the number of observations over the total, bucketed accordingly) where the distribution is centred around a central value without any bias towards the left or right, and the peak is at the centre.\n",
    "The random variables are distributed in the form of a symmetrical bell shaped curve.\n",
    "\n",
    "### Explain what a p-value is\n",
    "**As a simple definition: the likelihood of an observed statistic occuring due to chance, given the sampling distribution**\n",
    "\n",
    "*Also:\n",
    "The probability of obtaining the observed results of a test with the assumption that the null hypothesis is correct (therefore the probability that the null hypothesis is true).*\n",
    "\n",
    "A smaller p-value signifies stronger evidence of an alternative hypothesis.\n",
    "\n",
    "### What is a confidence interval?\n",
    "\n",
    "This is a type of interval estimate containing a range of values of the unknown population parameter.\n",
    "\n",
    "**Confidence level** representes the frequency of possible confidence intervals containing the true value of the population parameter.\n",
    "If confidence intervals are constructed using a given confidence level from an infinite number of sample statistics, the proportion of those intervals containing the true value of the parameter will be equal to the confidence level.\n",
    "\n",
    "### What is a Box-Cox transformation?\n",
    "\n",
    "Sometimes the **target variable** for a regression might not satisfy the criteria for a least squares regression- the residuals could curve as prediction increases or follow a skewed distribution.\n",
    "\n",
    "In these scenarios, it is necessary to apply a transformation to the response variable/outcome so the data meets the assumptions.\n",
    "\n",
    "A **Box Cox transformation** is a statistical technique to transform non-normal dependent variables into a normal shape (since most of the tests assume normality) - therefore increasing the scope of eligibility for a broad number of tests.\n",
    "\n",
    "### What is selection bias?\n",
    "\n",
    "**Selection bias** is when the sample obtained is not representative of the population intended to be analysed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
